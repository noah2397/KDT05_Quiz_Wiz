{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a49f8cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Korpora import Korpora\n",
    "from konlpy.tag import Okt\n",
    "# 일본어 tokenize import\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "\n",
    "corpus = pd.read_csv(\"./Data/TOTAL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08c857be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        写真帳」を英語でいうと何 ? loose leaf account book nOte al...\n",
      "1        鮭の内臓をとって、塩漬けにしたものを何という ? さらまき鮭 はらまき鮭 からまき鮮 あらま...\n",
      "2        次のうち、約21kmの距離で行われるマラソン競技はどれ ?  _ フルマフソン ハーフマラフ...\n",
      "3          秘の暑い時に、風を起こすために用いるものはどれ ?2 うちきさ うちわ うちこ うちみ うちわ\n",
      "4        関門海峡を挟んで九州と面している山口県の都市はどこ ? 上関市 下関市 右関市\"6 左関市 下関市\n",
      "                               ...                        \n",
      "12331                 北海道出身の芸能人は? 新坦結衣 里田まい 久本雅美 釈由美子 里田まい\n",
      "12332    「恋のマイレージ』などのヒット曲がある4人組男性アカペラボーカルグループは何 ? オフコース...\n",
      "12333    次のうち、ラジオ体操第一で最初に行う運動はどれ2 体をねじる運動 両脚で蹴ぶ運動 のびの連動...\n",
      "12334    次のうち、イタリアの物理学者トリチェリが、大気圧の実験をする際に使った液体はどれ ? 食塩水...\n",
      "12335    日本語\\r\\n語では圏谷\\r\\n谷ともいう、氷河によってできたお板上の谷を何という? ポテト...\n",
      "Name: combined, Length: 12336, dtype: object\n"
     ]
    }
   ],
   "source": [
    "corpus['combined'] = corpus.apply(lambda row: ' '.join(row), axis=1)\n",
    "print(corpus['combined'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd387120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표면형: すもも, 품사: 名詞,一般,*,*\n",
      "표면형: も, 품사: 助詞,係助詞,*,*\n",
      "표면형: もも, 품사: 名詞,一般,*,*\n",
      "표면형: も, 품사: 助詞,係助詞,*,*\n",
      "표면형: もも, 품사: 名詞,一般,*,*\n",
      "표면형: の, 품사: 助詞,連体化,*,*\n",
      "표면형: うち, 품사: 名詞,非自立,副詞可能,*\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 형태소 분석 실행\u001b[39;00m\n\u001b[0;32m     13\u001b[0m analyze_japanese_text(japanese_text)\n\u001b[1;32m---> 15\u001b[0m tokens_jp \u001b[38;5;241m=\u001b[39m [Tokenizer()\u001b[38;5;241m.\u001b[39mtokenize(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m corpus[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "Cell \u001b[1;32mIn[9], line 15\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 형태소 분석 실행\u001b[39;00m\n\u001b[0;32m     13\u001b[0m analyze_japanese_text(japanese_text)\n\u001b[1;32m---> 15\u001b[0m tokens_jp \u001b[38;5;241m=\u001b[39m [\u001b[43mTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtokenize(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m corpus[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\janome\\tokenizer.py:182\u001b[0m, in \u001b[0;36mTokenizer.__init__\u001b[1;34m(self, udic, udic_enc, udic_type, max_unknown_length, wakati, mmap, dotfile)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_dic: Optional[Union[UserDictionary, CompiledUserDictionary]]\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwakati \u001b[38;5;241m=\u001b[39m wakati\n\u001b[1;32m--> 182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatcher \u001b[38;5;241m=\u001b[39m Matcher(\u001b[43mall_fstdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msys_dic \u001b[38;5;241m=\u001b[39m MMapSystemDictionary\u001b[38;5;241m.\u001b[39minstance()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\janome\\sysdic\\__init__.py:104\u001b[0m, in \u001b[0;36mall_fstdata\u001b[1;34m()\u001b[0m\n\u001b[0;32m    102\u001b[0m res\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m    103\u001b[0m res\u001b[38;5;241m.\u001b[39mappend(base64\u001b[38;5;241m.\u001b[39mb64decode(fst_data0\u001b[38;5;241m.\u001b[39mDATA))\n\u001b[1;32m--> 104\u001b[0m res\u001b[38;5;241m.\u001b[39mappend(\u001b[43mbase64\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb64decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfst_data1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDATA\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\TEXT_017_220_38\\lib\\base64.py:87\u001b[0m, in \u001b[0;36mb64decode\u001b[1;34m(s, altchars, validate)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39mfullmatch(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[A-Za-z0-9+/]*=\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m0,2}\u001b[39m\u001b[38;5;124m'\u001b[39m, s):\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m binascii\u001b[38;5;241m.\u001b[39mError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNon-base64 digit found\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinascii\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ma2b_base64\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "def analyze_japanese_text(text):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    for token in tokens:\n",
    "        print(\"표면형: {}, 품사: {}\".format(token.surface, token.part_of_speech))\n",
    "\n",
    "# 예제 텍스트\n",
    "japanese_text = \"すもももももももものうち\"\n",
    "\n",
    "# 형태소 분석 실행\n",
    "analyze_japanese_text(japanese_text)\n",
    "\n",
    "tokens_jp = [Tokenizer().tokenize(text) for text in corpus['combined']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c121d6cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'generator' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[1;32m----> 4\u001b[0m word2vec \u001b[38;5;241m=\u001b[39m \u001b[43mWord2Vec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43msentences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens_jp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_final_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\TEXT_017_220_38\\lib\\site-packages\\gensim\\models\\word2vec.py:429\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m corpus_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, passes\u001b[38;5;241m=\u001b[39m(epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 429\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrim_rule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrim_rule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[0;32m    431\u001b[0m         corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, total_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_count,\n\u001b[0;32m    432\u001b[0m         total_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_total_words, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs, start_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha,\n\u001b[0;32m    433\u001b[0m         end_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_alpha, compute_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\TEXT_017_220_38\\lib\\site-packages\\gensim\\models\\word2vec.py:491\u001b[0m, in \u001b[0;36mWord2Vec.build_vocab\u001b[1;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Build vocabulary from a sequence of sentences (can be a once-only generator stream).\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \n\u001b[0;32m    455\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    488\u001b[0m \n\u001b[0;32m    489\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, passes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 491\u001b[0m total_words, corpus_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_vocab\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_per\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_per\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrim_rule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrim_rule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_count \u001b[38;5;241m=\u001b[39m corpus_count\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_total_words \u001b[38;5;241m=\u001b[39m total_words\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\TEXT_017_220_38\\lib\\site-packages\\gensim\\models\\word2vec.py:586\u001b[0m, in \u001b[0;36mWord2Vec.scan_vocab\u001b[1;34m(self, corpus_iterable, corpus_file, progress_per, workers, trim_rule)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_file:\n\u001b[0;32m    584\u001b[0m     corpus_iterable \u001b[38;5;241m=\u001b[39m LineSentence(corpus_file)\n\u001b[1;32m--> 586\u001b[0m total_words, corpus_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scan_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_per\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrim_rule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollected \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m word types from a corpus of \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m raw words and \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m sentences\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_vocab), total_words, corpus_count\n\u001b[0;32m    591\u001b[0m )\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_words, corpus_count\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\TEXT_017_220_38\\lib\\site-packages\\gensim\\models\\word2vec.py:571\u001b[0m, in \u001b[0;36mWord2Vec._scan_vocab\u001b[1;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sentence:\n\u001b[0;32m    570\u001b[0m     vocab[word] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 571\u001b[0m total_words \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_vocab_size \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(vocab) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_vocab_size:\n\u001b[0;32m    574\u001b[0m     utils\u001b[38;5;241m.\u001b[39mprune_vocab(vocab, min_reduce, trim_rule\u001b[38;5;241m=\u001b[39mtrim_rule)\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'generator' has no len()"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "word2vec = Word2Vec(\n",
    "    sentences=tokens_jp,\n",
    "    vector_size=128,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    sg=1,\n",
    "    epochs=3,\n",
    "    max_final_vocab=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b83369",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.save(\"./word2vec.model\")\n",
    "word2vec = Word2Vec.load(\"./word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db1c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.13119747 -0.21706526 -0.16385366  0.23436876  0.32412457  0.02247319\n",
      "  0.27886495 -0.24873996 -0.21546207 -0.05908812 -0.02252374  0.3414345\n",
      "  0.03190593  0.3183351   0.14777143 -0.18721291  0.45446986 -0.1614984\n",
      " -0.33309618  0.17657453  0.05139109 -0.00399886 -0.4758178  -0.04493817\n",
      "  0.17060606 -0.4843953  -0.04458258 -0.26275736  0.19177613  0.00225488\n",
      " -0.11836413  0.242319    0.40727746  0.2228171  -0.13209377 -0.12771982\n",
      "  0.02346163 -0.03312085 -0.06283128 -0.0499238   0.10890405  0.19079073\n",
      " -0.34962898  0.11058794 -0.05270893  0.03476163 -0.16924869  0.34395844\n",
      "  0.01980914  0.09783165  0.19175847  0.2691738   0.21441141 -0.18789609\n",
      "  0.07704082  0.2684671   0.26516977 -0.08228908 -0.20107695  0.22169317\n",
      " -0.13494085  0.40851566  0.23997435 -0.00461242  0.2174143  -0.5013253\n",
      " -0.16981839  0.16065446 -0.28387034 -0.24362709 -0.2745273   0.13810486\n",
      "  0.08134507  0.33698583 -0.03502255  0.0434599  -0.33982426  0.27116823\n",
      " -0.40502268 -0.12354109  0.00168343  0.12359767 -0.04100967  0.23189242\n",
      " -0.25682026  0.08644726 -0.05434482  0.08409004  0.36943284  0.25765437\n",
      "  0.29283628 -0.31254798  0.16600344  0.02845708  0.06765921  0.15221843\n",
      " -0.12369256  0.0680701   0.28246412  0.21572776  0.00330437 -0.09128039\n",
      " -0.20173873 -0.10156629  0.14706463  0.05189596 -0.02825897  0.0833881\n",
      "  0.13281703  0.37148118 -0.05246192  0.0819854   0.11717392  0.18312366\n",
      " -0.6959188  -0.04743974  0.32062265  0.4848667  -0.3237125   0.30416492\n",
      "  0.1877088   0.15473604  0.47905356  0.4713289  -0.08129945 -0.02557737\n",
      "  0.3506195  -0.06794777]\n",
      "[('💖', 0.865932047367096), ('멋지신', 0.7457040548324585), ('🐶🧡', 0.7267171740531921), ('송강', 0.7040014863014221), ('진화', 0.6655195951461792)]\n",
      "0.43795922\n"
     ]
    }
   ],
   "source": [
    "word = \"마인크래프트\"\n",
    "print(word2vec.wv[word])\n",
    "print(word2vec.wv.most_similar(word, topn=5))\n",
    "print(word2vec.wv.similarity(w1=word, w2=\"연기력\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afbc109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
